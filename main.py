import os, whisper, yt_dlp, shutil, time
from openai import OpenAI
import gradio as gr
from pydub import AudioSegment

def call_ai_pipeline(client, harvest_mode, text_content):
    """æå–å‡ºçš„ AI å¤„ç†æ ¸å¿ƒé€»è¾‘ï¼Œç”¨äºè¢«æµæ°´çº¿é‡å¤è°ƒç”¨"""
    # Step B: æ ‡ç‚¹è¿˜åŸ
    clean_p = f"ä½ æ˜¯ä¸€ä½æ–‡å­—æ•´ç†å¸ˆã€‚è¯·å¯¹ä»¥ä¸‹åŸå§‹è¯­éŸ³æ–‡æœ¬è¿›è¡Œã€æ ‡ç‚¹è¿˜åŸã€‘å’Œã€é€»è¾‘åˆ†æ®µã€‘ï¼Œä¸¥ç¦åˆ å‡æˆ–æ¶¦è‰²åŸæ–‡è¯è¯­ï¼š\n\n{text_content}"
    clean_res = client.chat.completions.create(model="qwen-turbo", messages=[{"role": "user", "content": clean_p}])
    segmented_text = clean_res.choices[0].message.content

    # Step C: æ ¹æ®æ¨¡å¼è¾“å‡º
    if harvest_mode == "é€æ®µç¿»è¯‘å¯¹ç…§":
        trans_p = f"ä»»åŠ¡ï¼šæ–‡æœ¬é€æ®µç¿»è¯‘å¯¹ç…§ã€‚è¦æ±‚ï¼šæ ¼å¼ä¸º [åŸæ–‡æ®µè½]\n[ç¿»è¯‘æ®µè½]\n---ã€‚è¯·ç¿»è¯‘ä»¥ä¸‹å·²åˆ†æ®µæ–‡æœ¬ï¼Œç¡®ä¿ç¿»è¯‘ç²¾å‡†ä¸”ä¸æ”¹å†™åŸæ–‡ï¼š\n\n{segmented_text}"
        res = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": trans_p}])
        return res.choices[0].message.content
    elif harvest_mode == "é€»è¾‘å¤§çº²æ¨¡å¼":
        map_p = f"ä½ æ˜¯ä¸€ä½é€»è¾‘åˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹åˆ†æ®µæ–‡ç¨¿æå–å¤§çº²å’Œmermaid mindmapä»£ç ï¼Œæ ¸å¿ƒè®ºç‚¹ç”¨ã€Œã€åŒ…è£¹ï¼š\n\n{segmented_text}"
        res = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": map_p}])
        return res.choices[0].message.content
    else:
        return segmented_text

def process_all_in_one(api_key, input_content, harvest_mode):
    final_key = api_key if api_key and "sk-" in api_key else "sk-placeholder"
    if "placeholder" in final_key:
        yield "âŒ é”™è¯¯ï¼šè¯·åœ¨ç•Œé¢è¾“å…¥æœ‰æ•ˆçš„ API_KEY", None
        return
    if not input_content.strip():
        yield "âŒ é”™è¯¯ï¼šè¾“å…¥å†…å®¹ä¸èƒ½ä¸ºç©º", None
        return

    client = OpenAI(api_key=final_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
    all_summary_report = ""
    all_files = []
    lines = [l.strip() for l in input_content.split('\n') if l.strip()]

    if harvest_mode == "é›…æ€ä½œæ–‡æ•™ç»ƒ":
        yield "âœï¸ è€ƒå®˜æ­£åœ¨ç ”è¯»ä½ çš„å¤§ä½œ...", None
        ielts_p = f"ä»»åŠ¡ï¼šé›…æ€å‰è€ƒå®˜æ·±åº¦æ‰¹æ”¹...\n{input_content}" 
        try:
            res = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": ielts_p}])
            feedback = res.choices[0].message.content
            f_name = "é›…æ€ä½œæ–‡æ‰¹æ”¹æŠ¥å‘Š.txt"
            with open(f_name, "w", encoding="utf-8") as f: f.write(feedback)
            yield feedback, [f_name]
        except Exception as e: yield f"âŒ æ‰¹æ”¹å¤±è´¥: {str(e)}", None
        return

    for idx, url in enumerate(lines):
        header = f"### ğŸ“º æ­£åœ¨æ”¶å‰² ({idx+1}/{len(lines)}): {url}\n"
        yield all_summary_report + header + "æ­£åœ¨æ£€æŸ¥/å¯åŠ¨ä¸‹è½½...", all_files
        
        try:
            audio_file = f"temp_audio_{idx}.mp3"
            # 1. ä¸‹è½½ (å¦‚æœ mp3 å·²å­˜åœ¨åˆ™è·³è¿‡ä¸‹è½½)
            if not os.path.exists(audio_file):
                opts = {'format': 'mp3/bestaudio/best','outtmpl': f'temp_audio_{idx}.%(ext)s','postprocessors': [{'key': 'FFmpegExtractAudio','preferredcodec': 'mp3'}],'quiet': True}
                with yt_dlp.YoutubeDL(opts) as ydl: ydl.download([url])

            # 2. è§£æéŸ³é¢‘
            audio_data = AudioSegment.from_file(audio_file)
            duration_mins = len(audio_data) / (60 * 1000)
            chunk_length = 20 * 60 * 1000
            chunks = [audio_data[i:i + chunk_length] for i in range(0, len(audio_data), chunk_length)]
            
            video_combined_text = ""
            yield all_summary_report + header + f"ğŸ“¦ éŸ³é¢‘å…¨é•¿ {duration_mins:.1f} åˆ†é’Ÿï¼Œå…± {len(chunks)} æ®µã€‚æ­£åœ¨åŒæ­¥æ–­ç‚¹...", all_files

            import torch
            model = whisper.load_model("base", device="cuda" if torch.cuda.is_available() else "cpu")

            for c_idx, chunk in enumerate(chunks):
                tmp_out = f"video_{idx+1}_Part_{c_idx+1}.txt"
                segment_divider = f"\n\n--- ğŸ“œ ç¬¬ {c_idx+1} éƒ¨åˆ† (çº¦ {c_idx*20}-{(c_idx+1)*20}min) ---\n\n"

                # ã€æ–­ç‚¹ç»­ä¼ é€»è¾‘ã€‘ï¼šå¦‚æœè¯¥æ®µçš„æ–‡æœ¬å·²å­˜åœ¨ï¼Œç›´æ¥è¯»å–å¹¶è·³è¿‡
                if os.path.exists(tmp_out):
                    yield all_summary_report + header + f"â© æ£€æµ‹åˆ°ç¬¬ {c_idx+1}/{len(chunks)} æ®µå·²å®Œæˆï¼Œæ­£åœ¨ç§’é€ŸåŠ è½½...", all_files
                    with open(tmp_out, "r", encoding="utf-8") as f:
                        processed_chunk_text = f.read()
                    video_combined_text += segment_divider + processed_chunk_text
                    if tmp_out not in all_files: all_files.append(tmp_out)
                    continue 

                # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œåˆ™æ‰§è¡Œæ­£å¸¸æ”¶å‰²
                chunk_filename = f"temp_{idx}_{c_idx+1}.mp3"
                chunk.export(chunk_filename, format="mp3")
                
                yield all_summary_report + header + f"ğŸ™ï¸ [ç¬¬{c_idx+1}æ®µ] Whisper å¬å†™ä¸­...", all_files
                raw_chunk_text = model.transcribe(chunk_filename)['text']
                
                yield all_summary_report + header + f"âœï¸ [ç¬¬{c_idx+1}æ®µ] AI æ­£åœ¨å¤„ç†...", all_files
                processed_chunk_text = call_ai_pipeline(client, harvest_mode, raw_chunk_text)
                
                # ä¿å­˜è¿™ä¸€æ®µçš„ç»“æœåˆ°ç¡¬ç›˜
                with open(tmp_out, "w", encoding="utf-8") as f: f.write(processed_chunk_text)
                video_combined_text += segment_divider + processed_chunk_text
                all_files.append(tmp_out)
                
                yield all_summary_report + header + video_combined_text, all_files

            # 4. å…¨éƒ¨å®Œæˆåä¿å­˜æ€»æ–‡ä»¶
            final_out_f = f"video_{idx+1}_å®Œæ•´æ”¶å‰²ç¨¿.txt"
            with open(final_out_f, "w", encoding="utf-8") as f: f.write(video_combined_text)
            all_files.append(final_out_f)
            all_summary_report += f"\n---\n{header}\nâœ… è§†é¢‘å¤„ç†å®Œæˆï¼\n"
            yield all_summary_report, all_files

        except Exception as e:
            all_summary_report += f"\nâŒ æŠ¥é”™: {str(e)}\n"
            yield all_summary_report, all_files

    yield all_summary_report + "\n\nâœ… ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼", all_files

# --- ç•Œé¢éƒ¨åˆ† ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ“ å­¦æœ¯å¤šåŠŸèƒ½æ”¶å‰²æœº (æ–­ç‚¹ç»­ä¼ æµæ°´çº¿ç‰ˆ)")
    with gr.Row():
        with gr.Column(scale=1):
            api_input = gr.Textbox(label="API KEY", type="password")
            input_box = gr.Textbox(label="è¾“å…¥åŒºåŸŸ", lines=8, placeholder="è§†é¢‘å¡«é“¾æ¥ï¼›ä½œæ–‡å¡«åŸæ–‡")
            mode_radio = gr.Radio(
                choices=["é€»è¾‘å¤§çº²æ¨¡å¼", "é€æ®µæ•´ç†(ä¸ç¿»è¯‘)", "é€æ®µç¿»è¯‘å¯¹ç…§", "é›…æ€ä½œæ–‡æ•™ç»ƒ"], 
                value="é€æ®µæ•´ç†(ä¸ç¿»è¯‘)", 
                label="é€‰æ‹©ä½œæˆ˜æ¨¡å¼"
            )
            btn = gr.Button("ğŸš€ ç«‹å³å¤„ç†", variant="primary")
            clear_btn = gr.Button("ğŸ§¹ æ¸…ç†ç¼“å­˜æ–‡ä»¶ (å¤„ç†æ–°è§†é¢‘å‰ç‚¹è¿™ä¸ª)")
        with gr.Column(scale=2):
            file_output = gr.File(label="ğŸ“¥ ä¸‹è½½ç»“æœ (å«åˆ†æ®µç¨¿)", file_count="multiple")
            output_md = gr.Markdown(label="ğŸ“„ å®æ—¶æµå¼é¢„è§ˆ")
    
    btn.click(fn=process_all_in_one, inputs=[api_input, input_box, mode_radio], outputs=[output_md, file_output])
    
    def clear():
        # æ¸…ç†æ‰€æœ‰ä¸­é—´è¿‡ç¨‹æ–‡ä»¶
        for f in os.listdir():
            if f.startswith(("temp_", "video_")) or f.endswith((".mp3", ".txt", ".m4a")):
                try: os.remove(f)
                except: pass
        return "âœ¨ ç¼“å­˜å·²æ¸…ç©ºï¼Œå¯ä»¥å¼€å§‹å¤„ç†å…¨æ–°çš„è§†é¢‘é“¾æ¥äº†ã€‚", None
    clear_btn.click(fn=clear, outputs=[output_md, file_output])

if __name__ == "__main__":
    demo.launch(share=True, debug=True)
