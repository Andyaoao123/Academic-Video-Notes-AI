import os, whisper, yt_dlp, shutil, time
from openai import OpenAI
import gradio as gr
from pydub import AudioSegment  # æ–°å¢ï¼šç”¨äºå¤„ç†è¶…é•¿éŸ³é¢‘åˆ‡ç‰‡

def call_ai_pipeline(client, harvest_mode, text_content):
    """æå–å‡ºçš„ AI å¤„ç†æ ¸å¿ƒé€»è¾‘ï¼Œç”¨äºè¢«æµæ°´çº¿é‡å¤è°ƒç”¨"""
    # Step B: æ ‡ç‚¹è¿˜åŸ
    clean_p = f"ä½ æ˜¯ä¸€ä½æ–‡å­—æ•´ç†å¸ˆã€‚è¯·å¯¹ä»¥ä¸‹åŸå§‹è¯­éŸ³æ–‡æœ¬è¿›è¡Œã€æ ‡ç‚¹è¿˜åŸã€‘å’Œã€é€»è¾‘åˆ†æ®µã€‘ï¼Œä¸¥ç¦åˆ å‡æˆ–æ¶¦è‰²åŸæ–‡è¯è¯­ï¼š\n\n{text_content}"
    clean_res = client.chat.completions.create(model="qwen-turbo", messages=[{"role": "user", "content": clean_p}])
    segmented_text = clean_res.choices[0].message.content

    # Step C: æ ¹æ®æ¨¡å¼è¾“å‡º
    if harvest_mode == "é€æ®µç¿»è¯‘å¯¹ç…§":
        trans_p = f"ä»»åŠ¡ï¼šæ–‡æœ¬é€æ®µç¿»è¯‘å¯¹ç…§ã€‚è¦æ±‚ï¼šæ ¼å¼ä¸º [åŸæ–‡æ®µè½]\n[ç¿»è¯‘æ®µè½]\n---ã€‚è¯·ç¿»è¯‘ä»¥ä¸‹å·²åˆ†æ®µæ–‡æœ¬ï¼Œç¡®ä¿ç¿»è¯‘ç²¾å‡†ä¸”ä¸æ”¹å†™åŸæ–‡ï¼š\n\n{segmented_text}"
        res = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": trans_p}])
        return res.choices[0].message.content
    elif harvest_mode == "é€»è¾‘å¤§çº²æ¨¡å¼":
        map_p = f"ä½ æ˜¯ä¸€ä½é€»è¾‘åˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹åˆ†æ®µæ–‡ç¨¿æå–å¤§çº²å’Œmermaid mindmapä»£ç ï¼Œæ ¸å¿ƒè®ºç‚¹ç”¨ã€Œã€åŒ…è£¹ï¼š\n\n{segmented_text}"
        res = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": map_p}])
        return res.choices[0].message.content
    else:
        return segmented_text

def process_all_in_one(api_key, input_content, harvest_mode):
    final_key = api_key if api_key and "sk-" in api_key else "sk-placeholder"
    if "placeholder" in final_key:
        yield "âŒ é”™è¯¯ï¼šè¯·åœ¨ç•Œé¢è¾“å…¥æœ‰æ•ˆçš„ API_KEY", None
        return
    if not input_content.strip():
        yield "âŒ é”™è¯¯ï¼šè¾“å…¥å†…å®¹ä¸èƒ½ä¸ºç©º", None
        return

    client = OpenAI(api_key=final_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
    all_summary_report = ""
    all_files = []
    lines = [l.strip() for l in input_content.split('\n') if l.strip()]

    # --- åœºæ™¯ Aï¼šé›…æ€ä½œæ–‡æ•™ç»ƒ (ä¿æŒä¸å˜) ---
    if harvest_mode == "é›…æ€ä½œæ–‡æ•™ç»ƒ":
        yield "âœï¸ è€ƒå®˜æ­£åœ¨ç ”è¯»ä½ çš„å¤§ä½œ...", None
        ielts_p = f"ä»»åŠ¡ï¼šé›…æ€å‰è€ƒå®˜æ·±åº¦æ‰¹æ”¹...\n{input_content}" # (æ­¤å¤„çœç•¥é•¿ Prompt ä¿æŒç®€æ´)
        try:
            res = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": ielts_p}])
            feedback = res.choices[0].message.content
            f_name = "é›…æ€ä½œæ–‡æ‰¹æ”¹æŠ¥å‘Š.txt"
            with open(f_name, "w", encoding="utf-8") as f: f.write(feedback)
            yield feedback, [f_name]
        except Exception as e: yield f"âŒ æ‰¹æ”¹å¤±è´¥: {str(e)}", None
        return

    # --- åœºæ™¯ Bï¼šè§†é¢‘å¤„ç†æµæ°´çº¿ (æ–°å¢åˆ†æ®µé€»è¾‘) ---
    for idx, url in enumerate(lines):
        header = f"### ğŸ“º æ­£åœ¨æ”¶å‰² ({idx+1}/{len(lines)}): {url}\n"
        yield all_summary_report + header + "æ­£åœ¨å¯åŠ¨ä¸‹è½½...", all_files
        
        try:
            audio_file = f"temp_audio_{idx}.m4a"
            # 1. ä¸‹è½½
            if not os.path.exists(audio_file):
                opts = {'format': 'm4a/bestaudio/best','outtmpl': f'temp_audio_{idx}.%(ext)s','postprocessors': [{'key': 'FFmpegExtractAudio','preferredcodec': 'm4a'}],'quiet': True}
                with yt_dlp.YoutubeDL(opts) as ydl: ydl.download([url])

            # 2. åˆ¤æ–­é•¿åº¦å¹¶å¼€å¯æµæ°´çº¿
            audio_data = AudioSegment.from_file(audio_file)
            duration_mins = len(audio_data) / (60 * 1000)
            
            # è®¾å®šåˆ‡ç‰‡é•¿åº¦ï¼š20åˆ†é’Ÿ (1200000ms)
            chunk_length = 20 * 60 * 1000
            chunks = [audio_data[i:i + chunk_length] for i in range(0, len(audio_data), chunk_length)]
            
            video_combined_text = ""
            yield all_summary_report + header + f"ğŸ“¦ éŸ³é¢‘å…¨é•¿ {duration_mins:.1f} åˆ†é’Ÿï¼Œå·²åˆ‡åˆ†ä¸º {len(chunks)} æ®µï¼Œå¼€å§‹æµæ°´çº¿å¤„ç†...", all_files

            import torch
            model = whisper.load_model("base", device="cuda" if torch.cuda.is_available() else "cpu")

            for c_idx, chunk in enumerate(chunks):
                chunk_tag = f"P{c_idx+1}"
                chunk_filename = f"temp_{idx}_{chunk_tag}.m4a"
                chunk.export(chunk_filename, format="m4a")
                
                # ç”Ÿäº§è€…ï¼šå¬å†™
                yield all_summary_report + header + f"ğŸ™ï¸ [ç¬¬{c_idx+1}æ®µ] Whisper å¬å†™ä¸­...", all_files
                raw_chunk_text = model.transcribe(chunk_filename)['text']
                
                # æ¶ˆè´¹è€…ï¼šAI å¤„ç†
                yield all_summary_report + header + f"âœï¸ [ç¬¬{c_idx+1}æ®µ] AI æ­£åœ¨æ•´ç†/ç¿»è¯‘...", all_files
                processed_chunk_text = call_ai_pipeline(client, harvest_mode, raw_chunk_text)
                
                # å®æ—¶æ›´æ–°ç»“æœ
                segment_divider = f"\n\n--- ğŸ“œ ç¬¬ {c_idx+1} éƒ¨åˆ† (çº¦ {c_idx*20}-{(c_idx+1)*20}min) ---\n\n"
                video_combined_text += segment_divider + processed_chunk_text
                
                # æ¯å®Œæˆä¸€æ®µå°±ç”Ÿæˆä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ä¾›ä¸‹è½½ï¼Œé˜²æ­¢å´©æºƒ
                tmp_out = f"video_{idx+1}_Part_{c_idx+1}.txt"
                with open(tmp_out, "w", encoding="utf-8") as f: f.write(processed_chunk_text)
                all_files.append(tmp_out)
                
                # å®æ—¶é¢„è§ˆ
                yield all_summary_report + header + video_combined_text, all_files

            # 4. å…¨éƒ¨å®Œæˆåä¿å­˜æ€»æ–‡ä»¶
            final_out_f = f"video_{idx+1}_å®Œæ•´æ”¶å‰²ç¨¿.txt"
            with open(final_out_f, "w", encoding="utf-8") as f: f.write(video_combined_text)
            all_files.append(final_out_f)
            all_summary_report += f"\n---\n{header}\nâœ… å…¨é•¿è§†é¢‘å¤„ç†å®Œæˆï¼\n"
            yield all_summary_report, all_files

        except Exception as e:
            all_summary_report += f"\nâŒ æŠ¥é”™: {str(e)}\n"
            yield all_summary_report, all_files

    yield all_summary_report + "\n\nâœ… ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼", all_files

# --- ç•Œé¢éƒ¨åˆ† (ä¿æŒä¸å˜) ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ“ å­¦æœ¯/å†™ä½œå¤šåŠŸèƒ½å…¨èƒ½æ”¶å‰²æœº (æµæ°´çº¿ç‰ˆ)")
    with gr.Row():
        with gr.Column(scale=1):
            api_input = gr.Textbox(label="API KEY", type="password")
            input_box = gr.Textbox(label="è¾“å…¥åŒºåŸŸ", lines=8, placeholder="è§†é¢‘å¡«é“¾æ¥ï¼›ä½œæ–‡å¡«åŸæ–‡")
            mode_radio = gr.Radio(
                choices=["é€»è¾‘å¤§çº²æ¨¡å¼", "é€æ®µæ•´ç†(ä¸ç¿»è¯‘)", "é€æ®µç¿»è¯‘å¯¹ç…§", "é›…æ€ä½œæ–‡æ•™ç»ƒ"], 
                value="é€æ®µæ•´ç†(ä¸ç¿»è¯‘)", 
                label="é€‰æ‹©ä½œæˆ˜æ¨¡å¼"
            )
            btn = gr.Button("ğŸš€ ç«‹å³å¤„ç†", variant="primary")
            clear_btn = gr.Button("ğŸ§¹ æ¸…ç†ç¼“å­˜æ–‡ä»¶")
        with gr.Column(scale=2):
            file_output = gr.File(label="ğŸ“¥ ä¸‹è½½ç»“æœ (å«åˆ†æ®µç¨¿)", file_count="multiple")
            output_md = gr.Markdown(label="ğŸ“„ å®æ—¶æµå¼é¢„è§ˆ")
    
    btn.click(fn=process_all_in_one, inputs=[api_input, input_box, mode_radio], outputs=[output_md, file_output])
    def clear():
        for f in os.listdir():
            if f.endswith((".m4a", ".txt", ".mp4")): os.remove(f)
        return "âœ¨ ç¼“å­˜å·²æ¸…ç©ºã€‚", None
    clear_btn.click(fn=clear, outputs=[output_md, file_output])

if __name__ == "__main__":
    demo.launch(share=True, debug=True)
